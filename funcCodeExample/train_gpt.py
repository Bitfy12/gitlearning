import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from tqdm import tqdm  # âœ… è¿›åº¦æ¡æ˜¾ç¤ºåº“

# =========================================
# 1ï¸âƒ£ åŸºæœ¬è¶…å‚æ•°ä¸è®¾å¤‡é…ç½®
# =========================================
batch_size = 128              # æ¯ä¸ª batch ä¸­çš„æ ·æœ¬æ•°é‡
learning_rate = 0.01          # åˆå§‹å­¦ä¹ ç‡
num_epochs = 10               # è®­ç»ƒçš„æ€»è½®æ•°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # è‡ªåŠ¨é€‰æ‹© GPU æˆ– CPU

# =========================================
# 2ï¸âƒ£ æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =========================================
# transforms.Composeï¼šé¡ºåºæ‰§è¡Œå¤šä¸ªé¢„å¤„ç†æ“ä½œ
# transforms.Normalize(mean, std)ï¼šå°†åƒç´ å€¼æ ‡å‡†åŒ–åˆ° [-1,1] åŒºé—´ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
transform = transforms.Compose([
    transforms.ToTensor(),  # å°†PILå›¾ç‰‡è½¬æ¢ä¸ºTensorï¼Œå¹¶è‡ªåŠ¨ç¼©æ”¾åˆ° [0, 1]
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# CIFAR10ï¼š10ç±»å½©è‰²å›¾åƒæ•°æ®é›† (32x32)
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# DataLoaderï¼šå°†æ•°æ®é›†åˆ†æ‰¹æ‰“åŒ…ã€æ‰“ä¹±ï¼Œå¹¶æ”¯æŒå¤šçº¿ç¨‹åŠ è½½
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
# âš ï¸ æ³¨æ„ï¼š
# - è®­ç»ƒé›†ä¸€å®šè¦ shuffle=Trueï¼Œé˜²æ­¢æ¨¡å‹è®°å¿†æ•°æ®é¡ºåºã€‚
# - num_workers æ ¹æ® CPU æ ¸å¿ƒæ•°è®¾ç½®ï¼Œå¦åˆ™å¯èƒ½å¤ªæ…¢æˆ–å¡æ­»ã€‚

# =========================================
# 3ï¸âƒ£ å®šä¹‰æ¨¡å‹ç»“æ„ï¼ˆç®€å• CNNï¼‰
# =========================================
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # ä¸¤å±‚å·ç§¯ + æ± åŒ–
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   # è¾“å…¥é€šé“=3, è¾“å‡ºé€šé“=32
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 32â†’64
        self.pool = nn.MaxPool2d(2, 2)                            # ä¸‹é‡‡æ ·ï¼Œå°ºå¯¸å‡åŠ
        self.fc1 = nn.Linear(64 * 8 * 8, 128)                     # å…¨è¿æ¥å±‚
        self.fc2 = nn.Linear(128, 10)                             # è¾“å‡ºå±‚ï¼ˆ10 ç±»ï¼‰
        self.relu = nn.ReLU()                                     # æ¿€æ´»å‡½æ•° ReLU

    def forward(self, x):
        # å‰å‘ä¼ æ’­ï¼šå®šä¹‰æ•°æ®æµåŠ¨çš„è®¡ç®—è·¯å¾„
        x = self.pool(self.relu(self.conv1(x)))  # [B,3,32,32] â†’ [B,32,16,16]
        x = self.pool(self.relu(self.conv2(x)))  # [B,32,16,16] â†’ [B,64,8,8]
        x = x.view(x.size(0), -1)                # å±•å¹³ï¼šå˜æˆ [B, 64*8*8]
        x = self.relu(self.fc1(x))               # éšè—å±‚
        x = self.fc2(x)                          # è¾“å‡ºå±‚ (æœªç»è¿‡ softmax)
        return x

# å°†æ¨¡å‹åŠ è½½åˆ° GPUï¼ˆæˆ– CPUï¼‰
model = SimpleCNN().to(device)

# =========================================
# 4ï¸âƒ£ å®šä¹‰æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨
# =========================================
criterion = nn.CrossEntropyLoss()  # åˆ†ç±»ä»»åŠ¡çš„å¸¸ç”¨æŸå¤±å‡½æ•°

# SGD ä¼˜åŒ–å™¨ï¼šå¸¦ momentumï¼ˆåŠ¨é‡ï¼‰å’Œ weight_decayï¼ˆL2 æ­£åˆ™ï¼‰
optimizer = optim.SGD(
    model.parameters(), 
    lr=learning_rate, 
    momentum=0.9, 
    weight_decay=5e-4
)

# StepLRï¼šæ¯è¿‡ step_size ä¸ª epoch å­¦ä¹ ç‡ä¹˜ä»¥ gamma
scheduler = StepLR(optimizer, step_size=5, gamma=0.1)

# =========================================
# 5ï¸âƒ£ è®­ç»ƒä¸éªŒè¯å¾ªç¯ï¼ˆåŠ å…¥ tqdm è¿›åº¦æ¡ï¼‰
# =========================================
for epoch in range(num_epochs):
    # -------- è®­ç»ƒé˜¶æ®µ --------
    model.train()  # å¯ç”¨è®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ dropout / BN çš„æ›´æ–°ï¼‰
    running_loss = 0.0

    # tqdmï¼šåŒ…è£…è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œæ˜¾ç¤ºå®æ—¶è¿›åº¦æ¡
    progress_bar = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Training", leave=False)

    for inputs, targets in progress_bar:
        # å°†æ•°æ®æ¬åˆ° GPU
        inputs, targets = inputs.to(device), targets.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # ---- åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–° ----
        optimizer.zero_grad()   # âš ï¸ æ¸…ç©ºä¸Šä¸€æ¬¡ç´¯ç§¯çš„æ¢¯åº¦ï¼ˆå¿…é¡»ï¼ï¼‰
        loss.backward()         # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        optimizer.step()        # æ›´æ–°å‚æ•°ï¼ˆæ‰§è¡Œä¸€æ¬¡æ¢¯åº¦ä¸‹é™ï¼‰

        # ç´¯ç§¯æŸå¤±ï¼ˆç”¨äºè®¡ç®— epoch å¹³å‡å€¼ï¼‰
        running_loss += loss.item()

        # åœ¨ tqdm è¿›åº¦æ¡ä¸­å®æ—¶æ˜¾ç¤ºå½“å‰ batch çš„æŸå¤±ä¸å­¦ä¹ ç‡
        progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])

    # âš ï¸ æ³¨æ„ï¼šscheduler.step() ä¸€å®šè¦æ”¾åœ¨ optimizer.step() ä¹‹åï¼
    scheduler.step()

    # -------- éªŒè¯é˜¶æ®µ --------
    model.eval()  # æ¨ç†æ¨¡å¼ï¼ˆå†»ç»“ dropout ä¸ BN çš„å‡å€¼æ–¹å·®ï¼‰
    correct = 0
    total = 0
    val_loss = 0.0

    # tqdmï¼šåŒ…è£…éªŒè¯é›†ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
    val_bar = tqdm(test_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Validation", leave=False)

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜ + åŠ å¿«æ¨ç†
        for inputs, targets in val_bar:
            inputs, targets = inputs.to(device), targets.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()

            # è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            # æ›´æ–° tqdm è¿›åº¦æ¡ä¿¡æ¯
            val_bar.set_postfix(val_loss=loss.item())

    acc = 100. * correct / total  # éªŒè¯é›†å‡†ç¡®ç‡

    # æ¯ä¸ª epoch æ‰“å°ä¸€æ¬¡æ€»ç»“ä¿¡æ¯
    print(f"âœ… Epoch [{epoch+1}/{num_epochs}] "
          f"| Train Loss: {running_loss/len(train_loader):.4f} "
          f"| Val Loss: {val_loss/len(test_loader):.4f} "
          f"| Val Acc: {acc:.2f}% "
          f"| LR: {optimizer.param_groups[0]['lr']:.6f}")

print("ğŸ¯ Training complete.")
